{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>ENCODED_CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>Computer grid to help the world\\n\\nYour comput...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>Gadget growth fuels eco concerns\\n\\nTechnology...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tech</td>\n",
       "      <td>Sony wares win innovation award\\n\\nSony has ta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>New Year's texting breaks record\\n\\nA mobile p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech</td>\n",
       "      <td>Players sought for $1m prize\\n\\nUK gamers are ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                              title  \\\n",
       "0     tech  Computer grid to help the world\\n\\nYour comput...   \n",
       "1     tech  Gadget growth fuels eco concerns\\n\\nTechnology...   \n",
       "2     tech  Sony wares win innovation award\\n\\nSony has ta...   \n",
       "3     tech  New Year's texting breaks record\\n\\nA mobile p...   \n",
       "4     tech  Players sought for $1m prize\\n\\nUK gamers are ...   \n",
       "\n",
       "   ENCODED_CATEGORY  \n",
       "0                 4  \n",
       "1                 4  \n",
       "2                 4  \n",
       "3                 4  \n",
       "4                 4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "file = 'pklFiles/df.pkl'\n",
    "fileobj = open(file, 'rb')\n",
    "\n",
    "df = pickle.load(fileobj)\n",
    "\n",
    "fileobj.close()\n",
    "\n",
    "print(type(df))\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Headlines\n",
    "\n",
    "X  = df['title']\n",
    "\n",
    "#Encoded News Categories\n",
    "\n",
    "y = df['ENCODED_CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(shuffled_df) * split_ratio)\n",
    "\n",
    "X_train = shuffled_df['title'][:split_index]\n",
    "y_train = shuffled_df['ENCODED_CATEGORY'][:split_index]\n",
    "\n",
    "X_test = shuffled_df['title'][split_index:]\n",
    "y_test = shuffled_df['ENCODED_CATEGORY'][split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (2435,)\n",
      "Shape of y:  (2435,)\n",
      "\n",
      "\n",
      "Shape of X_train: (1948,)\n",
      "Shape of y_train: (1948,)\n",
      "\n",
      "\n",
      "Shape of X_test: (487,)\n",
      "Shape of y_test: (487,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Shape of X_train: \" +  str(X_train.shape))\n",
    "print(\"Shape of y_train: \" +  str(y_train.shape))\n",
    "print(\"\\n\")\n",
    "print(\"Shape of X_test: \" +  str(X_test.shape))\n",
    "print(\"Shape of y_test: \" +  str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection: TF-IDF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizer:\n",
    "    def __init__(self, max_df=0.95, min_df=2):\n",
    "        self.vocabulary = {}\n",
    "        self.document_count = None\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.idf = None\n",
    "        \n",
    "    def _build_vocabulary(self, documents):\n",
    "        doc_freq = defaultdict(int)\n",
    "        term_freq = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            if pd.isna(doc):\n",
    "                continue\n",
    "                \n",
    "            words = str(doc).lower().split()\n",
    "            seen_words = set()\n",
    "            \n",
    "            for word in words:\n",
    "                term_freq[doc_idx][word] += 1\n",
    "                if word not in seen_words:\n",
    "                    doc_freq[word] += 1\n",
    "                    seen_words.add(word)\n",
    "        \n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        valid_terms = {\n",
    "            term: freq for term, freq in doc_freq.items()\n",
    "            if self.min_df <= freq <= n_docs * self.max_df\n",
    "        }\n",
    "        \n",
    "        self.vocabulary = {term: idx for idx, term in enumerate(valid_terms)}\n",
    "        self.document_count = n_docs\n",
    "        \n",
    "        self.idf = {\n",
    "            term: np.log(n_docs / (freq + 1)) + 1\n",
    "            for term, freq in doc_freq.items()\n",
    "            if term in self.vocabulary\n",
    "        }\n",
    "        \n",
    "        return term_freq\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        term_freq = self._build_vocabulary(documents)\n",
    "        return self._create_tfidf_matrix(documents, term_freq)\n",
    "    \n",
    "    def _create_tfidf_matrix(self, documents, term_freq):\n",
    "        n_docs = len(documents)\n",
    "        n_terms = len(self.vocabulary)\n",
    "        matrix = np.zeros((n_docs, n_terms))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            if pd.isna(doc):\n",
    "                continue\n",
    "                \n",
    "            doc_terms = term_freq[doc_idx]\n",
    "            max_freq = max(doc_terms.values()) if doc_terms else 1\n",
    "            \n",
    "            for term, freq in doc_terms.items():\n",
    "                if term in self.vocabulary:\n",
    "                    term_idx = self.vocabulary[term]\n",
    "                    tf = 0.5 + 0.5 * (freq / max_freq)  # normalized tf\n",
    "                    matrix[doc_idx, term_idx] = tf * self.idf[term]\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        term_freq = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            if pd.isna(doc):\n",
    "                continue\n",
    "                \n",
    "            words = str(doc).lower().split()\n",
    "            for word in words:\n",
    "                term_freq[doc_idx][word] += 1\n",
    "                \n",
    "        return self._create_tfidf_matrix(documents, term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1948, 4193)\n",
      "Test set shape: (487, 4193)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.7)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train_tfidf.shape)\n",
    "print(\"Test set shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01, n_iters=100):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        batch_size = 32\n",
    "        for _ in range(self.n_iters):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_indices = indices[i:min(i + batch_size, n_samples)]\n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y_[batch_indices]\n",
    "                \n",
    "                margin = y_batch * (np.dot(X_batch, self.w) - self.b)\n",
    "                mask = margin < 1\n",
    "                \n",
    "                dw = self.lambda_param * self.w\n",
    "                dw -= np.sum(X_batch[mask] * y_batch[mask, np.newaxis], axis=0)\n",
    "                self.w -= self.lr * dw / batch_size\n",
    "                \n",
    "                db = -np.sum(y_batch[mask])\n",
    "                self.b -= self.lr * db / batch_size\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for class 0\n",
      "Training classifier for class 1\n",
      "Training classifier for class 2\n",
      "Training classifier for class 3\n",
      "Training classifier for class 4\n",
      "Training classifier for class 5\n",
      "\n",
      "Support Vector Machine (TF-IDF Approach)\n",
      "Accuracy Score: 81.11%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[108   0   0   1   5   0]\n",
      " [ 18  66   0   2   0   0]\n",
      " [ 15   5  67   0   1   0]\n",
      " [  8   3   0  86   0   0]\n",
      " [ 25   5   3   0  68   0]\n",
      " [  1   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "n_classes = len(np.unique(y_train))\n",
    "classifiers = []\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'lambda_param': 0.01,\n",
    "    'n_iters': 100\n",
    "}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    print(f\"Training classifier for class {i}\")\n",
    "    binary_y = np.where(y_train == i, 1, -1)\n",
    "    \n",
    "    svm = SVM(**params)\n",
    "    svm.fit(X_train_tfidf, binary_y)\n",
    "    classifiers.append(svm)\n",
    "\n",
    "def predict_multiclass(X):\n",
    "    predictions = np.zeros((X.shape[0], n_classes))\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        predictions[:, i] = clf.predict(X)\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "pred = predict_multiclass(X_test_tfidf)\n",
    "\n",
    "print(\"\\nSupport Vector Machine (TF-IDF Approach)\")\n",
    "print(f\"Accuracy Score: {metrics.accuracy_score(y_test, pred):.2%}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection : Bag of Words(BOW) Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (BOW Approach)\n",
      "\n",
      "Accuracy Score: 87.68%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[92  1  4  4 13  0]\n",
      " [ 2 73  3  4  4  0]\n",
      " [ 0  0 85  3  0  0]\n",
      " [ 0  0  1 95  1  0]\n",
      " [ 7  4  3  5 82  0]\n",
      " [ 0  0  0  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "X_train_clean = X_train.fillna('')\n",
    "X_test_clean = X_test.fillna('')\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "count_X_train = count_vectorizer.fit_transform(X_train_clean)\n",
    "count_X_test = count_vectorizer.transform(X_test_clean)\n",
    "\n",
    "with open('pklFiles/count_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(count_vectorizer, file)\n",
    "\n",
    "svm_classifier = SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "svm_classifier.fit(count_X_train, y_train)\n",
    "\n",
    "with open('pklFiles/svm_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(svm_classifier, file)\n",
    "\n",
    "pred = svm_classifier.predict(count_X_test)\n",
    "\n",
    "print(\"Support Vector Machine (BOW Approach)\\n\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, pred):.2%}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "count_vectorizer = pickle.load(open('pklFiles/count_vectorizer.pkl', 'rb'))\n",
    "svm_classifier = pickle.load(open('pklFiles/svm_classifier.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category:  sport\n"
     ]
    }
   ],
   "source": [
    "encoded_categories = {\n",
    "    0: 'business',\n",
    "    1: 'entertainment',\n",
    "    2: 'politics',\n",
    "    3: 'sport',\n",
    "    4: 'tech'\n",
    "}\n",
    "\n",
    "user_headline = [input(\"Enter News Headline: \")]\n",
    "headline_count = count_vectorizer.transform(user_headline)\n",
    "\n",
    "pred = svm_classifier.predict(headline_count)\n",
    "print(\"Predicted Category: \", encoded_categories[pred[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
